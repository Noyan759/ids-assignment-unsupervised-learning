{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Second Part of the Assignment of IDS 2020-2021\n",
    "In this Jupyter notebook, please, document your results and the way you have obtained them. Please use the _Python environment_ provided at the beginning of the course. In addition to the _Jupyter notebook_, please submit _one zip-file_ containing all datasets and other outputs you have generated (such as pdf, jpg, and others). Please make sure that the datasets and other outputs are easily identifiable, i.e. use names as requested in the corresponding question.\n",
    "\n",
    "This is the _only_ submission that is required (Jupyter notebook + zip-file). A separate report is _not_ needed and will not be considered for grading. \n",
    "\n",
    "Give your commented Python code and answers in the corresponding provided cells. Make sure to answer all questions in a clear and explicit manner and discuss your outputs. _Please do not change the general structure of this notebook_. You can, however, add additional markdown or code cells if necessary. <b>Please DO NOT CLEAR THE OUTPUT of the notebook you are submitting! </b>\n",
    "\n",
    "It is not needed that the group members be the same as the group members of the first part of the assignment, <font color=\"red\"> *Please make sure to include names and matriculation numbers of all group members in the slot provided below.* </font> If a name or a student's matriculation number is missing, the student will not receive any points.\n",
    "\n",
    "Hint 1: While working on the assignment, you will get a better understanding of the datasets. Feel free to generate additional results and visualizations to support your answers. For example, this might be useful regarding data modification and simplification. <font color=\"red\">Ensure that all your claims are supported.</font>\n",
    "\n",
    "Hint 2: <font color=\"red\">Plan your time wisely. </font> A few parts of this assignment might take some time to run. It might be necessary to consider time management when you plan your group work.\n",
    "\n",
    "Hint 3: RWTHmoodle allows multiple submissions, with every new submission overwriting the previous one. <b>Partial submissions are therefore possible and encouraged. </b> This might be helpful in case of technical issues with RWTHMoodle, which may occur close to the deadline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"red\"><b>Student Names and matriculation numbers:\n",
    "    \n",
    "    1. \n",
    "    \n",
    "    2. \n",
    "    \n",
    "    3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1 - Data Preprocessing and Data Quality (15 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(a) Carry out the following preprocessing steps before starting the analysis:\n",
    "\n",
    "Select 90% of dataset <b>dataPrepViz.csv</b> for this assignment by random sampling. Use the matriculation number of one of the group members as seed. Rename the sampled dataset to <b>dataPrepViz_sampled</b> and export it as CSV.\n",
    "\n",
    " - <font color='red'>Important!</font> Make sure that you submit your extracted dataset with your results in Moodle.\n",
    "\n",
    "Use this dataset <b>dataPrepViz_sampled</b> as starting point for Question 1 and Question 2. Then apply further modifications as specified in the those questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create new dataset <b>data1</b> by removing the feature 'geographic_group' from <b>dataPrepViz_sampled</b>. Use this <b>data1</b> dataset for Question 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(b) We want to get a first impression of the data. To achieve this, compute and show the following:\n",
    "\n",
    "- the column names (the names of the features)\n",
    "- the data type of each feature\n",
    "- for categorical features: the number of classes and the value of the most frequent class\n",
    "- for numerical features: the mean, standard deviation, minimum and maximum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(c) For each feature, provide a histogram (with at least 10 bins each) showing the value distribution. Can you spot any obvious data quality issues, e.g. inconsistencies, implausible values or missing values (without researching on specific domain knowledge)?\n",
    "\n",
    "Briefly explain the issues you identified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explanation: \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(d) Substitute all implausible values as missing data (numpy.nan). Show the scatter matrix of the resulting dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(e) We need to handle any implausible or missing data. In the lecture, several strategies to do so have been introduced. \n",
    "\n",
    "In this question, consider implausible values to be the ones identified in part (c).\n",
    "\n",
    "    1) For all numerical features, compute and show mean, standard deviation, minimum and maximum, while ignoring the missing and implausible values. Also, print the total number of data rows.\n",
    "    \n",
    "    2) Based on the information obtained in the previous subtasks of this question, choose a strategy for handling all missing/implausible values, such that\n",
    "    - for one feature, you delete all data rows that include a missing value.\n",
    "    - for one feature, you replace all missing values by the median value.\n",
    "    - for one feature, you impute the values based on other, continous features using a regression classifier.\n",
    "    Create a cleaned dataset with all those values handled accordingly. \n",
    "    \n",
    "    3) For all numerical features, compute and show mean, standard deviation, minimum and maximum with respect to your cleaned dataset. Also print the total number of data rows.\n",
    "    \n",
    "    4) Motivate and explain the choices you made in 2). Compare the computed statistical values before and after cleaning and briefly describe and evaluate any changes.\n",
    " \n",
    "*Hint: There might not be an obvious choice for the best strategy. In this case, sound reasoning based on correct observations is more important than the decision itself.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explanation: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2 - Data Preprocessing and Advanced Visualization (15 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this question, use the <b>dataPrepViz_sampled</b> dataset you created in Q1, part (a)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(a) To create a suitable input for the following questions, modify the dataset as listed below:\n",
    "\n",
    "    1) remove rows that contain negative values\n",
    "    2) remove all rows that contain missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(b) For this subtask remove the feature 'country' from the data. Create four parallel coordinate plots that visualize the relation between the numerical attributes for all geographic groups.\n",
    "\n",
    "    1) For the first parallel coordinate plot, use the values unchanged.\n",
    "    \n",
    "    2-4) For the remaining 3 parallel coordinate plots, first normalize all numerical attributes by mapping them individually to the interval between 0 and 1, that is, apply Min-max normalization. Draw the three plots with different orderings of the features (randomized or chosen by interest)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(c) For each of the questions 1-3 below:\n",
    "- Indicate all of your parallel coordinate plots, which are suitable for finding an answer to the question. Explain your selection. \n",
    "- If possible, briefly answer the questions.\n",
    "\n",
    "    1) Is there a correlation between fertility and CO2 emissions? If yes, is it positive or negative?\n",
    "\n",
    "    2) Is there a correlation between life expectancy and vaccination confidence? If yes, is it positive or negative?\n",
    "\n",
    "    3) Is there a correlation between CO2 emissions and perceived corruption? If yes, is it positive or negative?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explanation: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(d) In this subtask we prepare the data for the heat map, which we create in subtask (e). \n",
    "\n",
    "The heat map should visualize the vaccination confidence ('vccin_effect_dag') for different combinations of CO2 emissions ('co2_emissions_tonnes_per_person') and fertility ('children_per_woman_total_fertility'). The heatmap should have 40 columns and 40 rows. The shown vaccination confidence value should be the *median* of all values for each combination of CO2 emissions and fertility. \n",
    "\n",
    "Do the following steps in preparation:\n",
    "\n",
    "    1) First, drop all columns that are not needed in this task.\n",
    "\n",
    "    2) Discretize the CO2 emissions and fertility data into 40 bins each, using equal-width binning.\n",
    "\n",
    "    3) Group the data by CO2 emissions and fertility, using median to aggregate the values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(e) Use the modified data to create a heat map as specified in part (d). Answer the following questions based on that heat map and briefly explain how you derived your answer:\n",
    "\n",
    "    1) Which combination of bins results in the highest vaccination confidence? \n",
    "    \n",
    "    2) How do you explain empty fields in your heat map?\n",
    "    \n",
    "    3) Can you identify any pattern in the heat map, e.g. in the coloring or in the distribution of empty fields? What can be a possible reason for this?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explanation: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3 - Frequent Item Sets and Association Rules (15 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(a) Carry out some preprocessing steps before starting the analysis:\n",
    " - Select 90% of the <b>store_data</b> dataset by random sampling. Use the matriculation number of one of the group members as seed.\n",
    " - After completing this preprocessing step, export your final dataset as <b>store_data_2.csv</b> dataset and use it for the next steps of the assignment.\n",
    " - <font color='red'>Important!</font> Make sure that you submit your extracted dataset with your results in Moodle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# Step 1: Read the CSV file\n",
    "df_store_data = pd.read_csv('datasets/store_data.csv')\n",
    "# Step 2: Select 90% from the dataset by random sampling, with seed = \n",
    "df_store_data_sampled = df_store_data.sample(frac=0.9, random_state=416804)\n",
    "# Step3: Export the result to csv file\n",
    "df_store_data_sampled.to_csv ('datasets/store_data_2.csv', index = False, header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(b) Find the most frequent itemsets with the support of more than 0.04 using the Apriori algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=========== Apriori - Frequent itemsets with minimum support of 0.04 ===========\n",
      "     support                      itemsets\n",
      "0   0.087394                     (burgers)\n",
      "1   0.079099                        (cake)\n",
      "2   0.046956                   (champagne)\n",
      "3   0.058065                     (chicken)\n",
      "4   0.164420                   (chocolate)\n",
      "5   0.081321                     (cookies)\n",
      "6   0.050215                 (cooking oil)\n",
      "7   0.181010                        (eggs)\n",
      "8   0.079544                    (escalope)\n",
      "9   0.172715                (french fries)\n",
      "10  0.042957                 (fresh bread)\n",
      "11  0.063546             (frozen smoothie)\n",
      "12  0.095690           (frozen vegetables)\n",
      "13  0.050955               (grated cheese)\n",
      "14  0.132573                   (green tea)\n",
      "15  0.097467                 (ground beef)\n",
      "16  0.049178               (herb & pepper)\n",
      "17  0.046364                       (honey)\n",
      "18  0.077470              (low fat yogurt)\n",
      "19  0.129462                        (milk)\n",
      "20  0.237446               (mineral water)\n",
      "21  0.066361                   (olive oil)\n",
      "22  0.094653                    (pancakes)\n",
      "23  0.043549                      (salmon)\n",
      "24  0.071693                      (shrimp)\n",
      "25  0.050955                        (soup)\n",
      "26  0.173900                   (spaghetti)\n",
      "27  0.067694                    (tomatoes)\n",
      "28  0.063250                      (turkey)\n",
      "29  0.058658            (whole wheat rice)\n",
      "30  0.053325    (mineral water, chocolate)\n",
      "31  0.050215         (mineral water, eggs)\n",
      "32  0.040290  (mineral water, ground beef)\n",
      "33  0.048585         (mineral water, milk)\n",
      "34  0.058214    (mineral water, spaghetti)\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "from mlxtend.preprocessing import TransactionEncoder\n",
    "from mlxtend.frequent_patterns import apriori\n",
    "\n",
    "# Read data from file store_data.csv and\n",
    "store_data = []\n",
    "with open(\"datasets/store_data_2.csv\") as csvFile:\n",
    "    reader = csv.reader(csvFile)\n",
    "    for row in reader:\n",
    "        store_data.append(list(filter(None, row)))\n",
    "\n",
    "# Use TransactionEncoder module to convert an array to DataFrame for Apriori algorithm in mlxtend\n",
    "te = TransactionEncoder()\n",
    "te_ary = te.fit(store_data).transform(store_data)\n",
    "data = pd.DataFrame(te_ary, columns=te.columns_)\n",
    "\n",
    "# Use Apriori algorithm from mlxtend\n",
    "frequent_itemsets_apriori = apriori(data, min_support=0.04, use_colnames=True)\n",
    "print('\\n=========== Apriori - Frequent itemsets with minimum support of 0.04 ===========')\n",
    "print(frequent_itemsets_apriori)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(c) Find the most frequent itemsets with more than 1 member and a support of more than 0.04 using the Apriori algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=========== Apriori - Frequent itemsets of length > 1 ===========\n",
      "     support                      itemsets  length\n",
      "30  0.053325    (mineral water, chocolate)       2\n",
      "31  0.050215         (mineral water, eggs)       2\n",
      "32  0.040290  (mineral water, ground beef)       2\n",
      "33  0.048585         (mineral water, milk)       2\n",
      "34  0.058214    (mineral water, spaghetti)       2\n"
     ]
    }
   ],
   "source": [
    "frequent_itemsets_apriori['length'] = frequent_itemsets_apriori['itemsets'].apply(\n",
    "    lambda x: len(x))\n",
    "print('\\n=========== Apriori - Frequent itemsets of length > 1 ===========')\n",
    "print(frequent_itemsets_apriori[(frequent_itemsets_apriori['length'] > 1)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(d) Find the itemsets having min_confidence=0.3 and min_lift=1.2. Print support, confidence, and lift of the filtered rules in one table. How do you interpret the quality of the discovered rules?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=========== Apriori - rules_apriori with min_support:0.04 min_confidence:0.3 min_lift:1.2 ===========\n",
      "     antecedents      consequents  antecedent support  consequent support  \\\n",
      "1    (chocolate)  (mineral water)            0.164420            0.237446   \n",
      "3  (ground beef)  (mineral water)            0.097467            0.237446   \n",
      "5         (milk)  (mineral water)            0.129462            0.237446   \n",
      "7    (spaghetti)  (mineral water)            0.173900            0.237446   \n",
      "\n",
      "    support  confidence      lift  leverage  conviction  \n",
      "1  0.053325    0.324324  1.365885  0.014284    1.128579  \n",
      "3  0.040290    0.413374  1.740915  0.017147    1.299897  \n",
      "5  0.048585    0.375286  1.580509  0.017845    1.220645  \n",
      "7  0.058214    0.334753  1.409805  0.016922    1.146271  \n"
     ]
    }
   ],
   "source": [
    "from mlxtend.frequent_patterns import association_rules\n",
    "rules_apriori = association_rules(frequent_itemsets_apriori,\n",
    "                                  metric=\"lift\", min_threshold=1.2)\n",
    "print('\\n=========== Apriori - rules_apriori with min_support:0.04 min_confidence:0.3 min_lift:1.2 ===========')\n",
    "print(rules_apriori[(rules_apriori['confidence'] > 0.3)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explanation:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(e) Apply the FP-Growth algorithm for all the settings of b, c, and d."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=========== FP-Growth - Frequent itemsets with minimum support of 0.04 ===========\n",
      "     support                      itemsets\n",
      "0   0.237446               (mineral water)\n",
      "1   0.132573                   (green tea)\n",
      "2   0.077470              (low fat yogurt)\n",
      "3   0.071693                      (shrimp)\n",
      "4   0.066361                   (olive oil)\n",
      "5   0.063546             (frozen smoothie)\n",
      "6   0.046364                       (honey)\n",
      "7   0.043549                      (salmon)\n",
      "8   0.172715                (french fries)\n",
      "9   0.173900                   (spaghetti)\n",
      "10  0.079099                        (cake)\n",
      "11  0.181010                        (eggs)\n",
      "12  0.129462                        (milk)\n",
      "13  0.087394                     (burgers)\n",
      "14  0.164420                   (chocolate)\n",
      "15  0.067694                    (tomatoes)\n",
      "16  0.058065                     (chicken)\n",
      "17  0.095690           (frozen vegetables)\n",
      "18  0.097467                 (ground beef)\n",
      "19  0.081321                     (cookies)\n",
      "20  0.079544                    (escalope)\n",
      "21  0.094653                    (pancakes)\n",
      "22  0.058658            (whole wheat rice)\n",
      "23  0.063250                      (turkey)\n",
      "24  0.042957                 (fresh bread)\n",
      "25  0.049178               (herb & pepper)\n",
      "26  0.050955               (grated cheese)\n",
      "27  0.050215                 (cooking oil)\n",
      "28  0.050955                        (soup)\n",
      "29  0.046956                   (champagne)\n",
      "30  0.058214    (mineral water, spaghetti)\n",
      "31  0.050215         (mineral water, eggs)\n",
      "32  0.048585         (mineral water, milk)\n",
      "33  0.053325    (mineral water, chocolate)\n",
      "34  0.040290  (mineral water, ground beef)\n",
      "\n",
      "=========== FP-Growth - Frequent itemsets of length > 1 ===========\n",
      "     support                      itemsets  length\n",
      "30  0.058214    (mineral water, spaghetti)       2\n",
      "31  0.050215         (mineral water, eggs)       2\n",
      "32  0.048585         (mineral water, milk)       2\n",
      "33  0.053325    (mineral water, chocolate)       2\n",
      "34  0.040290  (mineral water, ground beef)       2\n",
      "\n",
      "=========== FP-Growth - rules_fpgrowth with min_support:0.04 min_confidence:0.3 min_lift:1.2 ===========\n",
      "     antecedents      consequents  antecedent support  consequent support  \\\n",
      "1    (spaghetti)  (mineral water)            0.173900            0.237446   \n",
      "3         (milk)  (mineral water)            0.129462            0.237446   \n",
      "5    (chocolate)  (mineral water)            0.164420            0.237446   \n",
      "7  (ground beef)  (mineral water)            0.097467            0.237446   \n",
      "\n",
      "    support  confidence      lift  leverage  conviction  \n",
      "1  0.058214    0.334753  1.409805  0.016922    1.146271  \n",
      "3  0.048585    0.375286  1.580509  0.017845    1.220645  \n",
      "5  0.053325    0.324324  1.365885  0.014284    1.128579  \n",
      "7  0.040290    0.413374  1.740915  0.017147    1.299897  \n"
     ]
    }
   ],
   "source": [
    "from mlxtend.frequent_patterns import fpgrowth\n",
    "frequent_itemsets_fpgrowth = fpgrowth(\n",
    "    data, min_support=0.04, use_colnames=True)\n",
    "print('\\n=========== FP-Growth - Frequent itemsets with minimum support of 0.04 ===========')\n",
    "print(frequent_itemsets_fpgrowth)\n",
    "\n",
    "frequent_itemsets_fpgrowth['length'] = frequent_itemsets_fpgrowth['itemsets'].apply(\n",
    "    lambda x: len(x))\n",
    "print('\\n=========== FP-Growth - Frequent itemsets of length > 1 ===========')\n",
    "print(frequent_itemsets_fpgrowth[(frequent_itemsets_fpgrowth['length'] > 1)])\n",
    "\n",
    "rules_fpgrowth = association_rules(frequent_itemsets_fpgrowth,\n",
    "                                   metric=\"lift\", min_threshold=1.2)\n",
    "print('\\n=========== FP-Growth - rules_fpgrowth with min_support:0.04 min_confidence:0.3 min_lift:1.2 ===========')\n",
    "print(rules_fpgrowth[(rules_fpgrowth['confidence'] > 0.3)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4 - Text Mining (15 points): "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this question, we use <b>sms_data.csv</b>. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# your imports\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "import gensim\n",
    "from gensim.models import Doc2Vec\n",
    "import multiprocessing\n",
    "from tqdm import tqdm\n",
    "from sklearn import utils\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score, f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a) Load the dataset and create the <b>sampled_data</b> dataset which includes 90% of the data. Use the matriculation number of one of the group members as seed. Export the sampled dataset. Split the sampled data into training (80%) and test (20%) data preserving the distribution based on \"Label\".\n",
    "\n",
    "<font color='red'>Important!</font> Make sure that you submit your extracted dataset with your results in Moodle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create 90% sms_data_sampled from sms_data\n",
    "# Step 1: Read the CSV file\n",
    "df_sms_data_original = pd.read_csv(\n",
    "    'datasets/sms_data.csv', delimiter=\";\", encoding=\"unicode_escape\")\n",
    "# Step 2: Select 90% from the dataset by random sampling, with seed =\n",
    "df_sms_data_sampled = df_sms_data_original.sample(\n",
    "    frac=0.9, random_state=416808)\n",
    "# Step3: Export the result to csv file\n",
    "df_sms_data_sampled.to_csv(\n",
    "    'datasets/sms_data_sampled.csv', index=False, header=True, sep=\";\", encoding=\"unicode_escape\")\n",
    "\n",
    "# prepare the data\n",
    "input_data = df_sms_data_sampled.iloc[:, 1]\n",
    "target_data = df_sms_data_sampled.iloc[:, 0]\n",
    "\n",
    "# Split into 80% training and 20% test set\n",
    "series_input_train_data, series_input_test_data, series_target_train_data, series_target_test_data = train_test_split(input_data, target_data, test_size=0.2, stratify=target_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification\n",
    "\n",
    "In the following tasks, train each of the specified models with the training data and give for each the accuracy on the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(b) Model based on the binary document-term matrix\n",
    "\n",
    "Perform preprocessing on the training corpus (all lowercase, no punctuation, tokenization, stemming, and stopword removal) and obtain a binary document-term matrix. Train a logistic classifier with the 'Label' as target feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize and remove puntuation\n",
    "tokenizer = nltk.RegexpTokenizer(r\"\\w+\")\n",
    "# Stopwords\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "# Stemmer\n",
    "ps = PorterStemmer()\n",
    "\n",
    "def preProcessSeries(series):\n",
    "    for index, value in series.iteritems():\n",
    "        filtered_stemmed_words = []\n",
    "        for w in tokenizer.tokenize(series[index].lower()):\n",
    "            if w.lower() not in stop_words:\n",
    "                filtered_stemmed_words.append(ps.stem(w))\n",
    "        series[index] = ' '.join(filtered_stemmed_words)\n",
    "\n",
    "# Custom preprocess train data\n",
    "preProcessSeries(series_input_train_data)\n",
    "\n",
    "# Filtering out input and target features separately\n",
    "input_train_data = series_input_train_data.values\n",
    "target_train_data = series_target_train_data.values\n",
    "\n",
    "model1_classifier = Pipeline([('vect', CountVectorizer()), ('tf-idf', TfidfTransformer()),\n",
    "                     ('mlp-logistic', MLPClassifier(activation='logistic', random_state=42))])\n",
    "model1_classifier = model1_classifier.fit(input_train_data, target_train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(c) Model based on doc2vec\n",
    "\n",
    "- Perform preprocessing on the training corpus (all lowercase, no punctuation, tokenization, stemming, and stopword removal). \n",
    "- Obtain a doc2vec embedding in order to reduce the dimension of the document vector. Explain which vector size you use and why.\n",
    "- Use the doc2vec model you just trained to convert the training set to a set of document vectors.\n",
    "- Train a logistic classifier with 'Label' as target feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4012/4012 [00:00<00:00, 3538916.43it/s]\n",
      "100%|██████████| 4012/4012 [00:00<00:00, 3912473.30it/s]\n",
      "100%|██████████| 4012/4012 [00:00<00:00, 4101278.98it/s]\n",
      "100%|██████████| 4012/4012 [00:00<00:00, 3653397.23it/s]\n",
      "100%|██████████| 4012/4012 [00:00<00:00, 4092302.44it/s]\n",
      "100%|██████████| 4012/4012 [00:00<00:00, 4229089.63it/s]\n",
      "100%|██████████| 4012/4012 [00:00<00:00, 3432092.12it/s]\n",
      "100%|██████████| 4012/4012 [00:00<00:00, 4237609.58it/s]\n",
      "100%|██████████| 4012/4012 [00:00<00:00, 4089318.99it/s]\n",
      "100%|██████████| 4012/4012 [00:00<00:00, 4314755.81it/s]\n",
      "100%|██████████| 4012/4012 [00:00<00:00, 4180757.18it/s]\n",
      "100%|██████████| 4012/4012 [00:00<00:00, 4239744.94it/s]\n",
      "100%|██████████| 4012/4012 [00:00<00:00, 4522318.64it/s]\n",
      "100%|██████████| 4012/4012 [00:00<00:00, 4653635.96it/s]\n",
      "100%|██████████| 4012/4012 [00:00<00:00, 4643363.04it/s]\n",
      "100%|██████████| 4012/4012 [00:00<00:00, 4448201.86it/s]\n",
      "100%|██████████| 4012/4012 [00:00<00:00, 4362859.13it/s]\n",
      "100%|██████████| 4012/4012 [00:00<00:00, 3671731.98it/s]\n",
      "100%|██████████| 4012/4012 [00:00<00:00, 4648493.83it/s]\n",
      "100%|██████████| 4012/4012 [00:00<00:00, 4423645.54it/s]\n",
      "100%|██████████| 4012/4012 [00:00<00:00, 4022841.90it/s]\n",
      "100%|██████████| 4012/4012 [00:00<00:00, 4277465.09it/s]\n",
      "100%|██████████| 4012/4012 [00:00<00:00, 4697807.83it/s]\n",
      "100%|██████████| 4012/4012 [00:00<00:00, 4614079.42it/s]\n",
      "100%|██████████| 4012/4012 [00:00<00:00, 4695186.29it/s]\n",
      "100%|██████████| 4012/4012 [00:00<00:00, 4290552.69it/s]\n",
      "100%|██████████| 4012/4012 [00:00<00:00, 4693876.61it/s]\n",
      "100%|██████████| 4012/4012 [00:00<00:00, 4752202.10it/s]\n",
      "100%|██████████| 4012/4012 [00:00<00:00, 4288365.86it/s]\n",
      "100%|██████████| 4012/4012 [00:00<00:00, 4583913.82it/s]\n",
      "100%|██████████| 4012/4012 [00:00<00:00, 4647210.07it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='logistic', max_iter=1000, random_state=42)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenizing, normalizing, and creating lists of TaggedDocument objects\n",
    "tagged_train_data = []\n",
    "for i in range(0, len(input_train_data)):\n",
    "    tagged_train_data.append(gensim.models.doc2vec.TaggedDocument(words=gensim.utils.simple_preprocess(\n",
    "        input_train_data[i]), tags=[target_train_data[i]]))\n",
    "\n",
    "cores = multiprocessing.cpu_count()\n",
    "# Building the vocabulary\n",
    "doc2vec_model = Doc2Vec(dm=0, vector_size=40, workers=cores)\n",
    "doc2vec_model.build_vocab([x for x in tqdm(tagged_train_data)])\n",
    "# Training the doc2vec model\n",
    "for epoch in range(30):\n",
    "    doc2vec_model.train(utils.shuffle([x for x in tqdm(\n",
    "        tagged_train_data)]), total_examples=len(tagged_train_data), epochs=1)\n",
    "    doc2vec_model.alpha -= 0.002\n",
    "    doc2vec_model.min_alpha = doc2vec_model.alpha\n",
    "# Building the feature vector for the classifier\n",
    "def vec_for_learning(model, docs):\n",
    "    doc2vec_vectors = [model.infer_vector(doc.words) for doc in docs]\n",
    "    targets = [doc.tags[0] for doc in docs]\n",
    "    return doc2vec_vectors, targets\n",
    "# Translating docs into vectors for training and test set\n",
    "input_train_vectors, target_train_vectors = vec_for_learning(doc2vec_model, tagged_train_data)\n",
    "# Training a classification model\n",
    "model2_classifier = MLPClassifier(activation='logistic', random_state=42, max_iter=1000)\n",
    "model2_classifier.fit(input_train_vectors, target_train_vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation\n",
    "For the following tasks, use the test data.\n",
    "\n",
    "(d) Predict the classification with the two models on the test data. Preprocess the data if necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom preprocess data\n",
    "preProcessSeries(series_input_test_data)\n",
    "\n",
    "# Filtering out input and target features separately\n",
    "input_test_data = series_input_test_data.values\n",
    "target_test_data = series_target_test_data.values\n",
    "\n",
    "# Model 1\n",
    "# Predict the classification\n",
    "predicted_results_model1 = model1_classifier.predict(input_test_data)\n",
    "\n",
    "# Model 2\n",
    "# Tokenizing, normalizing, and creating lists of TaggedDocument objects\n",
    "tagged_test_data = []\n",
    "for i in range(0, len(input_test_data)):\n",
    "    tagged_test_data.append(gensim.models.doc2vec.TaggedDocument(words=gensim.utils.simple_preprocess(\n",
    "        input_test_data[i]), tags=[target_test_data[i]]))\n",
    "# Translating docs into vectors for training and test set\n",
    "input_test_vectors, target_test_vectors = vec_for_learning(doc2vec_model, tagged_test_data)\n",
    "# Predict the classification\n",
    "predicted_results_model2 = model2_classifier.predict(input_test_vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explanation:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(e) Obtain the confusion matrices for the two models and the prediction on the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----CONFUSION MATRICES-----\n",
      "\n",
      "==============\n",
      "Model 1\n",
      "==============\n",
      "Confusion matrix -> \n",
      " [[869   0]\n",
      " [ 14 120]]\n",
      "==============\n",
      "Model 2\n",
      "==============\n",
      "Confusion matrix -> \n",
      " [[849  20]\n",
      " [ 60  74]]\n",
      "-------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def printConfusionMatrices(inputModelSets):\n",
    "    confusionMatrices = []\n",
    "    print('\\n----CONFUSION MATRICES-----\\n')\n",
    "    for i in range(len(inputModelSets)):\n",
    "        print('==============')\n",
    "        print('Model %d' % (i+1))\n",
    "        print('==============')\n",
    "        print('Confusion matrix -> \\n', confusion_matrix(inputModelSets[i][0], inputModelSets[i][1]))\n",
    "    print('-------------\\n')\n",
    "\n",
    "printConfusionMatrices([[target_test_data, predicted_results_model1],[target_test_vectors, predicted_results_model2]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(f) Obtain accuracy and F1-score for the prediction of the two different models on the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 1 - Test accuracy 0.9860418743768694\n",
      "Model 1 - Test F1 score: 0.9857129703902351\n",
      "Model 2 - Test accuracy 0.9202392821535393\n",
      "Model 2 - Test F1 score: 0.9141399238827633\n"
     ]
    }
   ],
   "source": [
    "# Classification performance metrics for model 1\n",
    "print('Model 1 - Test accuracy %s' % accuracy_score(target_test_data, predicted_results_model1))\n",
    "print('Model 1 - Test F1 score: {}'.format(f1_score(target_test_data, predicted_results_model1, average='weighted')))\n",
    "# Classification performance metrics for model 2\n",
    "print('Model 2 - Test accuracy %s' % accuracy_score(target_test_vectors, predicted_results_model2))\n",
    "print('Model 2 - Test F1 score: {}'.format(f1_score(target_test_vectors, predicted_results_model2, average='weighted')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(g) Briefly comment on the quality of the two models. Interpret the results retrieved in the evaluation part."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explanation: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Language model\n",
    "\n",
    "For the following tasks use the <b>sampled_data</b>.\n",
    "\n",
    "(h) Create two lists, one for ham and one for spam, containing all messages.\n",
    "For ham and spam separately, build a bigram language model using the initial dataset (before splitting to training and test data). Do not perform stemming nor stopword removal for this task, but apply other preprocessing steps, such as all to lowercase, no punctuation and tokenization. Use both right and left padding, and manage unknown terms by using a dedicated token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(i) For each message groups, use the correspondent language model from (h) to generate, using MLE, a sentence of fifteen words using the following terms as seed:\n",
    "- 'hello'\n",
    "- 'yes'\n",
    "- 'but'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(j) Build a trigram model with the same data as in the previous task. Use both right and left padding, and manage unknown terms by using a dedicated token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(k) For each message group (ham and spam), use the correspondent language model from the previous qustion to generate, using MLE, a sentence of fifteen words using the following terms as seed:\n",
    "- 'hello'\n",
    "- 'yes'\n",
    "- 'but'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(l) Compare the quality of the generated text. Which model performs better? In general, which differences are there in using trigrams as opposed to bigrams?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explanation: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 5 - Process Mining (15 points): "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General investigation\n",
    "\n",
    "a) Import the event log from the <b>Quarantine_Log</b> csv file. Set the case ID to 'patient', Timestamp to 'timestamp' and Activity as 'activity'. Also, set the lifecyle column to the right attribute. Furthermore, identify the case attributes and set them to case attributes. Find the correct setting, so that the resource is understood as resource (compare with the documentation). Give some basic information:\n",
    "\n",
    "    - number of cases\n",
    "    - number of variants\n",
    "    - number of events\n",
    "    - the trace and event attribute names\n",
    "    - the number of resources\n",
    "    - the earliest timestamp and the latest timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sampling the event log\n",
    "(b) Create a sample of the event log (<b>log_sampled</b>) containing 80% of the traces. Export the sampled event log.\n",
    "\n",
    "<font color='red'>Important!</font> Make sure that you submit your extracted event log with your results in Moodle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trace frequency\n",
    "(c) Use the sampled event log and print the least frequent and the most frequent variant and the corresponding counts. Is there already some indication about the model structure (e.g. loops, parallel, etc.)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explanation: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter event logs\n",
    "(d) Create three different event logs:\n",
    "\n",
    "1. One event log containing only the 10% of the most frequent traces (**filtered_log_variants**).\n",
    "2. One event log containing only patients with private insurance (**filtered_log_insurance**).\n",
    "3. One event log containing only patients having the event attribute type as 'cloud' (**filtered_log_cloud**)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discovery and conformance checking\n",
    "\n",
    "(e) Use the Inductive Miner to discover a process model (Process tree or Petri net) for each event log created in (d). For one of the models - you may choose - explain shortly the behaviour of the model. (e.g. loops, sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explanation:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(f) Briefly summarize the differences and similarities of the models. Why do they differ/are similar?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explanation: \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(g) Perform the token-based replay for conformance checking using your discovered model for **filtered_log_variants** and the original event log. Does your process model fit the log? Explain the result in one sentence. Calculate the trace and log fitness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explanation: The model has a low fitness, because of the low number of traces used to create the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Frequency and performance\n",
    "\n",
    "(h) Visualize the model for the **filtered_log_variants** event log enriched with frequency information. Subsequently, visualize that same model enriched with performance information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(i) What are frequent activities? Why may they be frequent (think about the real life process described by the log)? What are possibly problematic activities according to the performance?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explanation: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 6 - Big Data (15 points): "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparation: generating a simple log\n",
    "\n",
    "In this question, we use the event log from the log csv file with the following modifications:\n",
    "1. We flatten the lifecycles (i.e., start and complete) into a single event. Each event contains the start timestamp and complete timestamp.\n",
    "2. A new column, called ServiceTime column, is included which represents the duration of the corresponding activity in the event.\n",
    "\n",
    "We name the event log as **simple_log** in the remainder. Please follow the explanations below to prepare the **simple_log**. The preparation steps will not be graded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We use following utility functions for the modifications (these are given):\n",
    "def _distinguish_duplicate_activities(log):\n",
    "    \"\"\"Add flags to the duplicate activities in a trace in order to distinguish them\n",
    "\n",
    "    Keyword arguments:\n",
    "    log -- even log\n",
    "    \"\"\"\n",
    "    trace = list()\n",
    "    activity_list = list()\n",
    "    count=0\n",
    "    prev_caseid=\"\"\n",
    "    for row in log.itertuples():\n",
    "        activity=row.Activity\n",
    "        caseid=row.Patient\n",
    "        if(caseid!=prev_caseid):\n",
    "            count=0\n",
    "            prev_caseid=caseid\n",
    "            trace=[]\n",
    "\n",
    "        if activity in trace:\n",
    "            count+=1\n",
    "            activity = activity + \"-{}\".format(count)\n",
    "            \n",
    "        trace.append(activity)\n",
    "        activity_list.append(activity)\n",
    "    log[\"Activity\"] = activity_list\n",
    "    return log\n",
    "\n",
    "def _merge_lifecylces(log):\n",
    "    \"\"\"Merge lifycycles (start,complete) into a single event. \n",
    "\n",
    "    Keyword arguments:\n",
    "    log -- even log\n",
    "    \"\"\"\n",
    "    start_log = log.loc[log[\"Lifecycle\"]==\"start\"]\n",
    "    start_log = _distinguish_duplicate_activities(start_log)\n",
    "    \n",
    "    complete_log = log.loc[log[\"Lifecycle\"]==\"complete\"]\n",
    "    complete_log = _distinguish_duplicate_activities(complete_log)\n",
    "\n",
    "    complete_log[\"CompleteTime\"] = complete_log[\"ModelTime\"]\n",
    "    simple_log = start_log.merge(complete_log, left_on=['Patient',\"Activity\"], right_on=['Patient',\"Activity\"],suffixes=(\"\", \"_y\"))\n",
    "    simple_log.drop(simple_log.filter(regex='_y$').columns.tolist(),axis=1, inplace=True)\n",
    "    simple_log[\"ServiceTime\"] = simple_log[\"CompleteTime\"] - simple_log[\"ModelTime\"]\n",
    "    return simple_log\n",
    "\n",
    "def produce_simple_log(filepath):\n",
    "    \"\"\"Produce simple log where the lifecycles are merged and service time information is added\n",
    "\n",
    "    Keyword arguments:\n",
    "    filepath -- path to event log\n",
    "    \"\"\"\n",
    "    log = pd.read_csv(filepath, sep=',')\n",
    "    log.sort_values(by=[\"Patient\",\"ModelTime\"],inplace=True)\n",
    "    simple_log = _merge_lifecylces(log)\n",
    "    return simple_log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Preparation step 1**: Replace the filepath to your own filepath to produce the **simple_log**.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "#your filepath\n",
    "filepath = \"./Datasets/Quarantine_log.csv\"\n",
    "simple_log = produce_simple_log(filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparation: expanding the event log\n",
    "\n",
    "In this question, we generate 100 event logs based on the <b>simple_log</b>. Each log replicates the base log (i.e., the <b>simple_log</b>). For randomization, you need to use the sum of the group's matriculation numbers (e.g., a group with 3 students having \"100000\", \"100001\", and \"100002\" as their matriculation numbers will use \"300003\" for the randomization)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We use following utility functions for the modifications (these are given):\n",
    "import random\n",
    "def _randomize_case_attribute(log,matriculation_num):\n",
    "    \"\"\"Randomize case attributes based on the matriculation number\n",
    "\n",
    "    Keyword arguments:\n",
    "    log -- event log\n",
    "    matriculation_num - sum of matriculation numbers\n",
    "    \"\"\"\n",
    "    random.seed(matriculation_num)\n",
    "    caseids = set(log[\"Patient\"])\n",
    "    for caseid in caseids:\n",
    "        random_val = random.randint(-3,3)\n",
    "        random.seed(random_val)\n",
    "        log.loc[log[\"Patient\"]==caseid,\"Age\"] = log.loc[log[\"Patient\"]==caseid,\"Age\"]+random_val\n",
    "    return log\n",
    "\n",
    "def _extract_log(log,iter_num):\n",
    "    \"\"\"Extract n-th log to ./generated_logs/\n",
    "\n",
    "    Keyword arguments:\n",
    "    log -- event log\n",
    "    iter_num -- n-th iteration\n",
    "    \"\"\"\n",
    "    log.to_csv(\"./generated_logs/generated_log-{}.tsv\".format(iter_num),header=False,index=False, sep=\"\\t\",line_terminator=\"\")\n",
    "\n",
    "def generate_log(original_log,num_replication,mat_num):\n",
    "    \"\"\"Generate logs (randomized by the matriculation number and extracted to ./generated_logs/) \n",
    "\n",
    "    Keyword arguments:\n",
    "    log -- event log\n",
    "    num_replication -- number of generated logs\n",
    "    mat_num -- sum of matriculation numbers\n",
    "    \"\"\"\n",
    "    import os\n",
    "    dir_path = \"./generated_logs\"\n",
    "    try:\n",
    "        os.mkdir(dir_path)\n",
    "    except OSError:\n",
    "        print (\"Directory already exists: %s\" % dir_path)\n",
    "    else:\n",
    "        print (\"Successfully created the directory %s \" % dir_path)\n",
    "    \n",
    "    base_log = original_log.copy(deep=True)\n",
    "    max_modeltime = max(base_log[\"ModelTime\"])\n",
    "    max_patientid = max(base_log[\"Patient\"])\n",
    "    for i in range(num_replication):\n",
    "        generated_log = base_log\n",
    "        generated_log[\"Patient\"] += max_patientid\n",
    "        generated_log[\"ModelTime\"] += max_modeltime\n",
    "        random.seed(None)\n",
    "        randomized_log = _randomize_case_attribute(generated_log,random.randint(0,mat_num))\n",
    "        _extract_log(randomized_log,i)\n",
    "        print (\"Successfully created %i th log at %s \"% (i,dir_path))\n",
    "        base_log = randomized_log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Preparation step 2**: Generate 100 replicated logs in your local disk (./generated-logs/generated-log-0.tsv, ./generated-logs/generated-log-1.tsv, ..., ./generated-logs/generated-log-99.tsv). Do not forget to replace the SUM_MAT_NUM to yours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#your group's sum\n",
    "SUM_MAT_NUM = 154031 \n",
    "base_log = simple_log[[\"Patient\", \"ModelTime\",\"Activity\",\"Age\",\"ServiceTime\"]] # this will be removed\n",
    "NUM_REPITITION=1\n",
    "generate_log(base_log,NUM_REPITITION,SUM_MAT_NUM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Hadoop\n",
    "\n",
    "(a) Now, it's time to work with the Hadoop system. The goal of this task is to merge 100 event logs at your disk in the Hadoop system. Follow the instructions below and show your results in each step (screenshots of the command line). We use \"letter identifier\" for this task (The letter identifier is the string consisting of the first letters of the group memebers' first names, e.g., for the group with \"Alessandro Berti\", \"Bernardo Silva\", \"Chiao Li\", the indentifier is \"ABC\").\n",
    "\n",
    "    1) Import the event logs to your Docker engine (at /usr/local/hadoop/(identifier)-generated-logs/).\n",
    "    2) Upload the files to the running Hadoop system (at /input/(identifier)-generated-logs/). \n",
    "    3) Merge the file and copy the result back to the Hadoop system (at /input/(identifier)-final-log.tsv).\n",
    "    4) Using the Hadoop command, print out the merged file in the command line (the screenshot may contain 10 rows)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#your code\n",
    "from IPython.display import Image\n",
    "Image(filename='your_path_to_screenshot_of_a1') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#your code\n",
    "Image(filename='your_path_to_screenshot_of_a2') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#your code\n",
    "Image(filename='your_path_to_screenshot_of_a3') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#your code\n",
    "Image(filename='your_path_to_screenshot_of_a4') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process Discovery\n",
    "\n",
    "(b) Discover a process model from the merged file using MapReduce algorithms. Explain how you discover the process model with the following deliverables:\n",
    "\n",
    "    1) Mapper function (as python file(s))\n",
    "    2) Reducer function (as python file(s))\n",
    "    3) Hadoop commands for MapReduce calculation (as text file)\n",
    "    4) Jupyter notebook code that prints the directly-follows relations and discover process models based on the directly-follows relations (you are free to use any discovery algorithms)\n",
    "\n",
    "<font color='red'>Important!</font> Please note that in this task, your result will be evaluated based on whether they are reproducible from your explanation. If you skip MapReduce calculations for this task, you will get 0 points.The deliverables of 1), 2), and 3) should be submitted as outputs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code for (b)-4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance Analysis\n",
    "\n",
    "(c) Calculate the total service time for each case using MapReduce algorithms. Explain how you calculate the total service time for each case with the following deliverables:\n",
    "\n",
    "    1) Mapper function (as python file(s))\n",
    "    2) Reducer function (as python file(s))\n",
    "    3) Hadoop commands for MapReduce calculation (as text file)\n",
    "    4) Result: total service times for cases (as text file)\n",
    "    \n",
    "Important! Please note that in this task, your result will be evaluated based on whether they are reproducible from your explanation. If you skip MapReduce calculations for this task, you will get 0 points.The deliverables of 1), 2), 3), and 4) should be submitted as outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(d) Visualize 1000 cases with the longest total service time using any chart."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
